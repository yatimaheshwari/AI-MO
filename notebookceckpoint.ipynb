{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt: use pretrained model gpt4 and train my data set\n",
    "You cannot use a pretrained model like GPT4 and train your own dataset in Google Colab or Jupyter Notebooks. You need to use a cloud-based platform like Google Cloud AI Platform or Amazon SageMaker to train your own dataset with GPT4.\n",
    "\n",
    "Here are the steps to train your own dataset with GPT4 on Google Cloud AI Platform.\n",
    "**Create a Cloud AI Platform project.**\n",
    "2. **Enable the Cloud AI Platform API.**\n",
    "3. **Create a dataset.**\n",
    "4. **Create a training pipeline.**\n",
    "5. **Train your model.**\n",
    "\n",
    "For more information, please refer to the following documentation:\n",
    "\n",
    "* [Train a text classification model with the gpt-4 model](https://cloud.google.com/ai-platform-unified/docs/training/train-text-classification-gpt-4)\n",
    "* [Google Cloud AI Platform documentation](https://cloud.google.com/ai-platform-unified/docs)\n",
    "\n",
    "Here are the steps to train your own dataset with GPT4 on Amazon SageMaker:\n",
    "\n",
    "1. **Create an Amazon SageMaker project.**\n",
    "2. **Enable the Amazon SageMaker API.**\n",
    "3. **Create a dataset.**\n",
    "4. **Create a training job.**\n",
    "5. **Train your model.**\n",
    "\n",
    "For more information, please refer to the following documentation:\n",
    "\n",
    "* [Train a text classification model with the gpt-4 model](https://docs.aws.amazon.com/sagemaker/latest/dg/text-classification-gpt4.html)\n",
    "* [Amazon SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the sum of the squares of the first 10...</td>\n",
       "      <td>The sum of the squares of the first n positive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>How many positive integers less than 1000 are ...</td>\n",
       "      <td>The least common multiple of 3 and 5 is 15. We...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The product of two numbers is 180, and their s...</td>\n",
       "      <td>Let the numbers be x and y such that xy = 180 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                            problem   \n",
       "0  1  What is the sum of the squares of the first 10...  \\\n",
       "1  2  How many positive integers less than 1000 are ...   \n",
       "2  3  The product of two numbers is 180, and their s...   \n",
       "\n",
       "                                              answer  \n",
       "0  The sum of the squares of the first n positive...  \n",
       "1  The least common multiple of 3 and 5 is 15. We...  \n",
       "2  Let the numbers be x and y such that xy = 180 ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the JSON data from file\n",
    "with open('package.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# print(data['problems'])\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data['problems'])\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "if major_version >= 8:\n",
    "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
    "else:\n",
    "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "# # https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
    "# project_id = '[your Cloud Platform project ID]'\n",
    "# !gcloud config set project {project_id}\n",
    "\n",
    "# # Install the PyTorch Lightning package.\n",
    "# !pip install pytorch-lightning\n",
    "\n",
    "# # Install the Transformers package.\n",
    "# !pip install transformers\n",
    "\n",
    "# # Import the necessary libraries.\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "# # Define the dataset class.\n",
    "# class CustomDataset(Dataset):\n",
    "#   def __init__(self, data):\n",
    "#     self.data = data\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.data)\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     return self.data[idx]\n",
    "\n",
    "# # Define the LightningModule class.\n",
    "# class GPT2FineTuner(LightningModule):\n",
    "#   def __init__(self, model_name, train_data, val_data, tokenizer):\n",
    "#     super().__init__()\n",
    "#     self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "#     self.train_data = train_data\n",
    "#     self.val_data = val_data\n",
    "#     self.tokenizer = tokenizer\n",
    "\n",
    "#   def forward(self, input_ids, labels):\n",
    "#     return self.model(input_ids=input_ids, labels=labels)\n",
    "\n",
    "#   def training_step(self, batch, batch_idx):\n",
    "#     input_ids, labels = batch\n",
    "#     output = self(input_ids, labels)\n",
    "#     loss = output.loss\n",
    "#     self.log('train_loss', loss)\n",
    "#     return loss\n",
    "\n",
    "#   def validation_step(self, batch, batch_idx):\n",
    "#     input_ids, labels = batch\n",
    "#     output = self(input_ids, labels)\n",
    "#     loss = output.loss\n",
    "#     self.log('val_loss', loss)\n",
    "\n",
    "#   def configure_optimizers(self):\n",
    "#     return torch.optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "\n",
    "# # Load the data.\n",
    "# train_data = pd.read_csv('train.csv')\n",
    "# val_data = pd.read_csv('val.csv')\n",
    "\n",
    "# # Tokenize the data.\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# train_data = tokenizer.batch_encode_plus(train_data['text'].tolist(), return_tensors='pt')['input_ids']\n",
    "# val_data = tokenizer.batch_encode_plus(val_data['text'].tolist(), return_tensors='pt')['input_ids']\n",
    "\n",
    "# # Create the datasets.\n",
    "# train_dataset = CustomDataset(train_data)\n",
    "# val_dataset = CustomDataset(val_data)\n",
    "\n",
    "# # Create the LightningModule.\n",
    "# model = GPT2FineTuner('gpt2', train_dataset, val_dataset, tokenizer)\n",
    "\n",
    "# # Train the model.\n",
    "# trainer = Trainer(gpus=1, max_epochs=10)\n",
    "# trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "# !pip install -q -U keras-nlp\n",
    "# !pip install -q -U keras>=3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
